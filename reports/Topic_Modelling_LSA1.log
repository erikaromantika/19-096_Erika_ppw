Traceback (most recent call last):
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python38-32\lib\site-packages\jupyter_cache\executors\utils.py", line 51, in single_nb_execution
    executenb(
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nbclient\client.py", line 1204, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nbclient\util.py", line 84, in wrapped
    return just_run(coro(*args, **kwargs))
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nbclient\util.py", line 62, in just_run
    return loop.run_until_complete(coro)
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python38-32\lib\asyncio\base_events.py", line 616, in run_until_complete
    return future.result()
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nbclient\client.py", line 663, in async_execute
    await self.async_execute_cell(
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nbclient\client.py", line 965, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "C:\Users\ASUS\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nbclient\client.py", line 862, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
# time taking
df['deskripsi_cleaned_text']=df['deskripsi'].apply(clean_text)
------------------

[1;31m---------------------------------------------------------------------------[0m
[1;31mTypeError[0m                                 Traceback (most recent call last)
Input [1;32mIn [19][0m, in [0;36m<cell line: 2>[1;34m()[0m
[0;32m      1[0m [38;5;66;03m# time taking[39;00m
[1;32m----> 2[0m df[[38;5;124m'[39m[38;5;124mdeskripsi_cleaned_text[39m[38;5;124m'[39m][38;5;241m=[39m[43mdf[49m[43m[[49m[38;5;124;43m'[39;49m[38;5;124;43mdeskripsi[39;49m[38;5;124;43m'[39;49m[43m][49m[38;5;241;43m.[39;49m[43mapply[49m[43m([49m[43mclean_text[49m[43m)[49m

File [1;32m~\AppData\Local\Programs\Python\Python38-32\lib\site-packages\pandas\core\series.py:4108[0m, in [0;36mSeries.apply[1;34m(self, func, convert_dtype, args, **kwds)[0m
[0;32m   4106[0m     [38;5;28;01melse[39;00m:
[0;32m   4107[0m         values [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mastype([38;5;28mobject[39m)[38;5;241m.[39m_values
[1;32m-> 4108[0m         mapped [38;5;241m=[39m [43mlib[49m[38;5;241;43m.[39;49m[43mmap_infer[49m[43m([49m[43mvalues[49m[43m,[49m[43m [49m[43mf[49m[43m,[49m[43m [49m[43mconvert[49m[38;5;241;43m=[39;49m[43mconvert_dtype[49m[43m)[49m
[0;32m   4110[0m [38;5;28;01mif[39;00m [38;5;28mlen[39m(mapped) [38;5;129;01mand[39;00m [38;5;28misinstance[39m(mapped[[38;5;241m0[39m], Series):
[0;32m   4111[0m     [38;5;66;03m# GH 25959 use pd.array instead of tolist[39;00m
[0;32m   4112[0m     [38;5;66;03m# so extension arrays can be used[39;00m
[0;32m   4113[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_constructor_expanddim(pd_array(mapped), index[38;5;241m=[39m[38;5;28mself[39m[38;5;241m.[39mindex)

File [1;32mpandas\_libs\lib.pyx:2467[0m, in [0;36mpandas._libs.lib.map_infer[1;34m()[0m

Input [1;32mIn [16][0m, in [0;36mclean_text[1;34m(headline)[0m
[0;32m      1[0m [38;5;28;01mdef[39;00m [38;5;21mclean_text[39m(headline):
[0;32m      2[0m   le[38;5;241m=[39mWordNetLemmatizer()
[1;32m----> 3[0m   word_tokens[38;5;241m=[39m[43mword_tokenize[49m[43m([49m[43mheadline[49m[43m)[49m
[0;32m      4[0m   tokens[38;5;241m=[39m[le[38;5;241m.[39mlemmatize(w) [38;5;28;01mfor[39;00m w [38;5;129;01min[39;00m word_tokens [38;5;28;01mif[39;00m w [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m stop_words [38;5;129;01mand[39;00m [38;5;28mlen[39m(w)[38;5;241m>[39m[38;5;241m3[39m]
[0;32m      5[0m   cleaned_text[38;5;241m=[39m[38;5;124m"[39m[38;5;124m [39m[38;5;124m"[39m[38;5;241m.[39mjoin(tokens)

File [1;32m~\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nltk\tokenize\__init__.py:129[0m, in [0;36mword_tokenize[1;34m(text, language, preserve_line)[0m
[0;32m    114[0m [38;5;28;01mdef[39;00m [38;5;21mword_tokenize[39m(text, language[38;5;241m=[39m[38;5;124m"[39m[38;5;124menglish[39m[38;5;124m"[39m, preserve_line[38;5;241m=[39m[38;5;28;01mFalse[39;00m):
[0;32m    115[0m     [38;5;124;03m"""[39;00m
[0;32m    116[0m [38;5;124;03m    Return a tokenized copy of *text*,[39;00m
[0;32m    117[0m [38;5;124;03m    using NLTK's recommended word tokenizer[39;00m
[1;32m   (...)[0m
[0;32m    127[0m [38;5;124;03m    :type preserve_line: bool[39;00m
[0;32m    128[0m [38;5;124;03m    """[39;00m
[1;32m--> 129[0m     sentences [38;5;241m=[39m [text] [38;5;28;01mif[39;00m preserve_line [38;5;28;01melse[39;00m [43msent_tokenize[49m[43m([49m[43mtext[49m[43m,[49m[43m [49m[43mlanguage[49m[43m)[49m
[0;32m    130[0m     [38;5;28;01mreturn[39;00m [
[0;32m    131[0m         token [38;5;28;01mfor[39;00m sent [38;5;129;01min[39;00m sentences [38;5;28;01mfor[39;00m token [38;5;129;01min[39;00m _treebank_word_tokenizer[38;5;241m.[39mtokenize(sent)
[0;32m    132[0m     ]

File [1;32m~\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nltk\tokenize\__init__.py:107[0m, in [0;36msent_tokenize[1;34m(text, language)[0m
[0;32m     97[0m [38;5;124;03m"""[39;00m
[0;32m     98[0m [38;5;124;03mReturn a sentence-tokenized copy of *text*,[39;00m
[0;32m     99[0m [38;5;124;03musing NLTK's recommended sentence tokenizer[39;00m
[1;32m   (...)[0m
[0;32m    104[0m [38;5;124;03m:param language: the model name in the Punkt corpus[39;00m
[0;32m    105[0m [38;5;124;03m"""[39;00m
[0;32m    106[0m tokenizer [38;5;241m=[39m load([38;5;124mf[39m[38;5;124m"[39m[38;5;124mtokenizers/punkt/[39m[38;5;132;01m{[39;00mlanguage[38;5;132;01m}[39;00m[38;5;124m.pickle[39m[38;5;124m"[39m)
[1;32m--> 107[0m [38;5;28;01mreturn[39;00m [43mtokenizer[49m[38;5;241;43m.[39;49m[43mtokenize[49m[43m([49m[43mtext[49m[43m)[49m

File [1;32m~\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nltk\tokenize\punkt.py:1276[0m, in [0;36mPunktSentenceTokenizer.tokenize[1;34m(self, text, realign_boundaries)[0m
[0;32m   1272[0m [38;5;28;01mdef[39;00m [38;5;21mtokenize[39m([38;5;28mself[39m, text, realign_boundaries[38;5;241m=[39m[38;5;28;01mTrue[39;00m):
[0;32m   1273[0m     [38;5;124;03m"""[39;00m
[0;32m   1274[0m [38;5;124;03m    Given a text, returns a list of the sentences in that text.[39;00m
[0;32m   1275[0m [38;5;124;03m    """[39;00m
[1;32m-> 1276[0m     [38;5;28;01mreturn[39;00m [38;5;28mlist[39m([38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43msentences_from_text[49m[43m([49m[43mtext[49m[43m,[49m[43m [49m[43mrealign_boundaries[49m[43m)[49m)

File [1;32m~\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nltk\tokenize\punkt.py:1332[0m, in [0;36mPunktSentenceTokenizer.sentences_from_text[1;34m(self, text, realign_boundaries)[0m
[0;32m   1325[0m [38;5;28;01mdef[39;00m [38;5;21msentences_from_text[39m([38;5;28mself[39m, text, realign_boundaries[38;5;241m=[39m[38;5;28;01mTrue[39;00m):
[0;32m   1326[0m     [38;5;124;03m"""[39;00m
[0;32m   1327[0m [38;5;124;03m    Given a text, generates the sentences in that text by only[39;00m
[0;32m   1328[0m [38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is[39;00m
[0;32m   1329[0m [38;5;124;03m    True, includes in the sentence closing punctuation that[39;00m
[0;32m   1330[0m [38;5;124;03m    follows the period.[39;00m
[0;32m   1331[0m [38;5;124;03m    """[39;00m
[1;32m-> 1332[0m     [38;5;28;01mreturn[39;00m [text[s:e] [38;5;28;01mfor[39;00m s, e [38;5;129;01min[39;00m [38;5;28mself[39m[38;5;241m.[39mspan_tokenize(text, realign_boundaries)]

File [1;32m~\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nltk\tokenize\punkt.py:1332[0m, in [0;36m<listcomp>[1;34m(.0)[0m
[0;32m   1325[0m [38;5;28;01mdef[39;00m [38;5;21msentences_from_text[39m([38;5;28mself[39m, text, realign_boundaries[38;5;241m=[39m[38;5;28;01mTrue[39;00m):
[0;32m   1326[0m     [38;5;124;03m"""[39;00m
[0;32m   1327[0m [38;5;124;03m    Given a text, generates the sentences in that text by only[39;00m
[0;32m   1328[0m [38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is[39;00m
[0;32m   1329[0m [38;5;124;03m    True, includes in the sentence closing punctuation that[39;00m
[0;32m   1330[0m [38;5;124;03m    follows the period.[39;00m
[0;32m   1331[0m [38;5;124;03m    """[39;00m
[1;32m-> 1332[0m     [38;5;28;01mreturn[39;00m [text[s:e] [38;5;28;01mfor[39;00m s, e [38;5;129;01min[39;00m [38;5;28mself[39m[38;5;241m.[39mspan_tokenize(text, realign_boundaries)]

File [1;32m~\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nltk\tokenize\punkt.py:1322[0m, in [0;36mPunktSentenceTokenizer.span_tokenize[1;34m(self, text, realign_boundaries)[0m
[0;32m   1320[0m [38;5;28;01mif[39;00m realign_boundaries:
[0;32m   1321[0m     slices [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39m_realign_boundaries(text, slices)
[1;32m-> 1322[0m [38;5;28;01mfor[39;00m sentence [38;5;129;01min[39;00m slices:
[0;32m   1323[0m     [38;5;28;01myield[39;00m (sentence[38;5;241m.[39mstart, sentence[38;5;241m.[39mstop)

File [1;32m~\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nltk\tokenize\punkt.py:1421[0m, in [0;36mPunktSentenceTokenizer._realign_boundaries[1;34m(self, text, slices)[0m
[0;32m   1408[0m [38;5;124;03m"""[39;00m
[0;32m   1409[0m [38;5;124;03mAttempts to realign punctuation that falls after the period but[39;00m
[0;32m   1410[0m [38;5;124;03mshould otherwise be included in the same sentence.[39;00m
[1;32m   (...)[0m
[0;32m   1418[0m [38;5;124;03m    ["(Sent1.)", "Sent2."].[39;00m
[0;32m   1419[0m [38;5;124;03m"""[39;00m
[0;32m   1420[0m realign [38;5;241m=[39m [38;5;241m0[39m
[1;32m-> 1421[0m [38;5;28;01mfor[39;00m sentence1, sentence2 [38;5;129;01min[39;00m _pair_iter(slices):
[0;32m   1422[0m     sentence1 [38;5;241m=[39m [38;5;28mslice[39m(sentence1[38;5;241m.[39mstart [38;5;241m+[39m realign, sentence1[38;5;241m.[39mstop)
[0;32m   1423[0m     [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m sentence2:

File [1;32m~\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nltk\tokenize\punkt.py:318[0m, in [0;36m_pair_iter[1;34m(iterator)[0m
[0;32m    316[0m iterator [38;5;241m=[39m [38;5;28miter[39m(iterator)
[0;32m    317[0m [38;5;28;01mtry[39;00m:
[1;32m--> 318[0m     prev [38;5;241m=[39m [38;5;28;43mnext[39;49m[43m([49m[43miterator[49m[43m)[49m
[0;32m    319[0m [38;5;28;01mexcept[39;00m [38;5;167;01mStopIteration[39;00m:
[0;32m    320[0m     [38;5;28;01mreturn[39;00m

File [1;32m~\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nltk\tokenize\punkt.py:1395[0m, in [0;36mPunktSentenceTokenizer._slices_from_text[1;34m(self, text)[0m
[0;32m   1393[0m [38;5;28;01mdef[39;00m [38;5;21m_slices_from_text[39m([38;5;28mself[39m, text):
[0;32m   1394[0m     last_break [38;5;241m=[39m [38;5;241m0[39m
[1;32m-> 1395[0m     [38;5;28;01mfor[39;00m match, context [38;5;129;01min[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_match_potential_end_contexts[49m[43m([49m[43mtext[49m[43m)[49m:
[0;32m   1396[0m         [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39mtext_contains_sentbreak(context):
[0;32m   1397[0m             [38;5;28;01myield[39;00m [38;5;28mslice[39m(last_break, match[38;5;241m.[39mend())

File [1;32m~\AppData\Local\Programs\Python\Python38-32\lib\site-packages\nltk\tokenize\punkt.py:1375[0m, in [0;36mPunktSentenceTokenizer._match_potential_end_contexts[1;34m(self, text)[0m
[0;32m   1373[0m before_words [38;5;241m=[39m {}
[0;32m   1374[0m matches [38;5;241m=[39m []
[1;32m-> 1375[0m [38;5;28;01mfor[39;00m match [38;5;129;01min[39;00m [38;5;28mreversed[39m([38;5;28mlist[39m([38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_lang_vars[49m[38;5;241;43m.[39;49m[43mperiod_context_re[49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[43mfinditer[49m[43m([49m[43mtext[49m[43m)[49m)):
[0;32m   1376[0m     [38;5;66;03m# Ignore matches that have already been captured by matches to the right of this match[39;00m
[0;32m   1377[0m     [38;5;28;01mif[39;00m matches [38;5;129;01mand[39;00m match[38;5;241m.[39mend() [38;5;241m>[39m before_start:
[0;32m   1378[0m         [38;5;28;01mcontinue[39;00m

[1;31mTypeError[0m: expected string or bytes-like object
TypeError: expected string or bytes-like object

